{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern,DotProduct\n",
    "#import grid search\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import make_scorer\n",
    "from sklearn.metrics import make_scorer\n",
    "import torch\n",
    "import esm\n",
    "#import pearsonr\n",
    "from scipy.stats import pearsonr\n",
    "import tqdm\n",
    "import random\n",
    "#import spearmanr\n",
    "from scipy.stats import spearmanr\n",
    "#import r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "from copy import deepcopy\n",
    "#close warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "##############\n",
    "sns.set_context('paper', font_scale=1.2)\n",
    "sns.set_style('white')\n",
    "#sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style({'font.family':'serif','font.serif':'Times New Roman'})\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = \"MTYKLIINGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\"\n",
    "sites = [\"V39\",\"D40\",\"G41\",\"V54\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RG = np.random.default_rng(42)\n",
    "def pi(y_pre,y_std):\n",
    "    from scipy.stats import norm\n",
    "    y_max = y_pre.max()\n",
    "    probs = norm.cdf((y_pre - y_max) / (y_std+1E-9))\n",
    "    return probs\n",
    "def ucb(y,y_std,beta=2):\n",
    "    # _y = y/y.max()\n",
    "    # _std = y_std/y_std.max()\n",
    "    return y + beta * y_std\n",
    "\n",
    "def random(Y_mean: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Random acquistion score\"\"\"\n",
    "    return RG.random(len(Y_mean))\n",
    "\n",
    "\n",
    "class Acquisiton:\n",
    "    def __init__(self, pool):\n",
    "        self.pool = pool\n",
    "\n",
    "    def random(self, sample_size=1000):\n",
    "\n",
    "        return self.pool.sample(n=sample_size)# random_state=42)\n",
    "\n",
    "    def entropy_sample(self, props, sample_size=500):\n",
    "        from scipy.stats import entropy\n",
    "\n",
    "        # props = props.reshape((-1, 1))\n",
    "        # props = np.hstack((1 - props, props))\n",
    "        entropies = entropy(props, axis=1, base=2)\n",
    "        _pool = self.pool\n",
    "        _pool[\"entropy\"] = entropies\n",
    "        _pool.sort_values(\"entropy\", inplace=True)\n",
    "        return _pool.iloc[-sample_size:]\n",
    "    \n",
    "    def ucb_sample(self,y_pre,y_std,sample_size=10,beta=2):\n",
    "        u = ucb(y=y_pre,y_std=y_std,beta=beta)\n",
    "        _pool = self.pool\n",
    "        _pool[\"u\"] = u\n",
    "        _pool.sort_values(\"u\", inplace=True)\n",
    "        return _pool.iloc[-sample_size:]\n",
    "    def pi_sample(self,y_pre,y_std,sample_size=10):\n",
    "        u = pi(y_pre=y_pre,y_std=y_std)\n",
    "        _pool = self.pool\n",
    "        _pool[\"u\"] = u\n",
    "        _pool.sort_values(\"u\", inplace=True)\n",
    "        return _pool.iloc[-sample_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = [\n",
    "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R',\n",
    "    'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "\n",
    "def get_esm(seq):\n",
    "    data = [('0', seq.strip())]\n",
    "\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    if torch.cuda.is_available():\n",
    "        batch_tokens = batch_tokens.to(device=\"cuda\", non_blocking=True)\n",
    "    batch_tokens = batch_tokens.reshape((1, -1))\n",
    "    # Extract per-residue representations\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1:tokens_len -\n",
    "                                                              1].mean(0))\n",
    "    return sequence_representations[0].cpu()\n",
    "\n",
    "#ref: Biswas, Surojit, et al. \"Low-N protein engineering with data-efficient deep learning.\" Nature methods 18.4 (2021): 389-396.\n",
    "\n",
    "def run_DE_trajectories(s_wt,\n",
    "                        Model,\n",
    "                        num_iterations,\n",
    "                        num_trajectories,\n",
    "                        T=0.01,\n",
    "                        trust_radius=12,\n",
    "                        fixed_sites=None):\n",
    "\n",
    "    s_records = []  # initialize list of sequence records\n",
    "    y_records = []  # initialize list of fitness score records\n",
    "\n",
    "    for i in tqdm.tqdm(range(num_trajectories)): #iterate through however many mutation trajectories we want to sample\n",
    "        s_traj, y_traj = directed_evolution(\n",
    "            s_wt=s_wt,\n",
    "            num_iterations=num_iterations,\n",
    "            Model=Model,\n",
    "            T=T,\n",
    "            trust_radius=trust_radius,\n",
    "            fixed_sites=fixed_sites\n",
    "        )  # call the directed evolution function, outputting the trajectory sequence and fitness score records\n",
    "\n",
    "        s_records.append(\n",
    "            s_traj\n",
    "        )  # update the sequence trajectory records for this full mutagenesis trajectory\n",
    "        y_records.append(\n",
    "            y_traj\n",
    "        )  # update the fitness trajectory records for this full mutagenesis trajectory\n",
    "    s_records = np.array(s_records)\n",
    "    y_records = np.array(y_records)\n",
    "\n",
    "    # plt.clf()\n",
    "    # fig = plt.figure(figsize=(10,6))\n",
    "    # plt.plot(np.transpose(y_records[:,:,0])) # plot the changes in fitness for all sampled trajectories\n",
    "    # plt.ylabel('Predicted Fitness')\n",
    "    # plt.xlabel('Mutation Trial Steps')\n",
    "    # plt.show() # show the plot :)\n",
    "\n",
    "    return s_records, y_records\n",
    "\n",
    "\n",
    "def directed_evolution(\n",
    "    s_wt,\n",
    "    num_iterations,\n",
    "    Model,\n",
    "    T=0.01,\n",
    "    trust_radius=12,\n",
    "    fixed_sites=None\n",
    "):  # input = (wild-type sequence, number of mutation iterations, \"temperature\")\n",
    "\n",
    "    s_traj = [\n",
    "    ]  # initialize an array to keep records of the protein sequences for this trajectory\n",
    "    y_traj = [\n",
    "    ]  # initialize an array to keep records of the fitness scores for this trajectory\n",
    "\n",
    "    mut_loc_seed = random.randint(\n",
    "        0, len(s_wt)\n",
    "    )  # randomely choose the location of the first mutation in the trajectory\n",
    "    s, new_mut_loc = mutate_sequence(\n",
    "        s_wt, (np.random.poisson(2) + 1),\n",
    "        mut_loc_seed,\n",
    "        fixed_sites=fixed_sites\n",
    "    )  # initial mutant sequence for this trajectory, with m = Poisson(2)+1 mutations\n",
    "\n",
    "    x = get_esm(s).reshape((1, -1))\n",
    "\n",
    "    y = Model.predict(\n",
    "        x\n",
    "    )  # predicted fitness score for the initial mutant sequence for this trajectory\n",
    "\n",
    "    # iterate through the trial mutation steps for the directed evolution trajectory\n",
    "    for i in range(num_iterations):\n",
    "        mu = np.random.uniform(\n",
    "            1, 2.5\n",
    "        )  # \"mu\" parameter for poisson function: used to control how many mutations to introduce\n",
    "        m = np.random.poisson(\n",
    "            mu -\n",
    "            1) + 1  # how many random mutations to apply to current sequence\n",
    "        s_new, new_mut_loc = mutate_sequence(\n",
    "            s, m, new_mut_loc, fixed_sites=fixed_sites\n",
    "        )  # new trial sequence, produced from \"m\" random mutations\n",
    "        if trust_radius:\n",
    "            truths = (np.array(list(s_new)) == np.array(list(s_wt)))\n",
    "            num_mut = np.count_nonzero(~truths)\n",
    "            if num_mut <= trust_radius:\n",
    "                x_new = get_esm(s_new).reshape((1, -1))\n",
    "                y_new = Model.predict(\n",
    "                    x_new)  # new fitness value for trial sequence\n",
    "            else:\n",
    "                y_new = -99999999\n",
    "        else:\n",
    "            x_new = get_esm(s_new).reshape((1, -1))\n",
    "            y_new = Model.predict(\n",
    "                x_new)  # new fitness value for trial sequence\n",
    "        with warnings.catch_warnings():  # catching Overflow warning\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            try:\n",
    "                p = min(1,\n",
    "                        np.exp((y_new - y) /\n",
    "                               T))  # probability function for trial sequence\n",
    "            except OverflowError:\n",
    "                p = 1\n",
    "        rand_var = random.random()\n",
    "\n",
    "        if rand_var < p:  # metropolis-Hastings update selection criterion\n",
    "            #print(str(new_mut_loc+1)+\" \"+s[new_mut_loc]+\"->\"+s_new[new_mut_loc])\n",
    "            s, y = s_new, y_new  # if criteria is met, update sequence and corresponding fitness\n",
    "\n",
    "        s_traj.append(\n",
    "            s\n",
    "        )  # update the sequence trajectory records for this iteration of mutagenesis\n",
    "        y_traj.append(\n",
    "            y\n",
    "        )  # update the fitness trajectory records for this iteration of mutagenesis\n",
    "\n",
    "    return s_traj, y_traj  # output = (sequence record for trajectory, fitness score recorf for trajectory)\n",
    "\n",
    "\n",
    "def mutate_sequence(\n",
    "    seq,\n",
    "    m,\n",
    "    prev_mut_loc,\n",
    "    fixed_sites=None\n",
    "):  # produce a mutant sequence (integer representation), given an initial sequence and the number of mutations to introduce (\"m\")\n",
    "\n",
    "    for i in range(m):  #iterate through number of mutations to add\n",
    "        if not fixed_sites:\n",
    "            rand_loc = random.randint(prev_mut_loc - 8, prev_mut_loc +\n",
    "                                      8)  # find random position to mutate\n",
    "            while (rand_loc <= 0) or (rand_loc >= len(seq)):\n",
    "                rand_loc = random.randint(prev_mut_loc - 8, prev_mut_loc + 8)\n",
    "\n",
    "            rand_aa = random.randint(0,\n",
    "                                     19)  # find random amino acid to mutate to\n",
    "            seq = list(seq)\n",
    "            seq[rand_loc] = amino_acids[\n",
    "                rand_aa]  # update sequence to have new amino acid at randomely chosen position\n",
    "            seq = ''.join(seq)\n",
    "        else:\n",
    "            positions = []\n",
    "            for mu_site in fixed_sites:\n",
    "                pos = int(mu_site[1:]) - 1\n",
    "                if pos not in positions:\n",
    "                    positions.append(pos)\n",
    "            rand_loc = random.choice(positions)\n",
    "            rand_aa = random.randint(0, 19)\n",
    "            seq = list(seq)\n",
    "            seq[rand_loc] = amino_acids[rand_aa]\n",
    "            seq = ''.join(seq)\n",
    "    return seq, rand_loc  # output the randomely mutated sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function report pearsonr,spearmanr,r2 print out\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from sklearn.metrics import r2_score\n",
    "def report(model,X_test,y_true):\n",
    "    y_pred = model.predict(X_test)\n",
    "    pearsonr_ = pearsonr(y_true, y_pred)\n",
    "    spearmanr_ = spearmanr(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print('Pearsonr: %.3f, p-value: %.3f' % (pearsonr_[0], pearsonr_[1]))\n",
    "    print('Spearmanr: %.3f, p-value: %.3f' % (spearmanr_[0], spearmanr_[1]))\n",
    "    print('R2: %.3f' % (r2))\n",
    "    plt.plot(y_true, y_pred, 'o')\n",
    "    plt.plot(np.linspace(y_true.min(), y_true.max(), 100), np.linspace(y_true.min(), y_true.max(), 100), 'r--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_focused_lib(wt,sites,model,num_iterations=30,num_trajectories=300,T=0.01,trust_radius=None):\n",
    "    num_iterations = num_iterations\n",
    "    num_trajectories = num_trajectories\n",
    "    seqs, ys = run_DE_trajectories(s_wt=wt,\n",
    "                                    Model=model,\n",
    "                                    num_iterations=num_iterations,\n",
    "                                    num_trajectories=num_trajectories,\n",
    "                                    T=0.01,\n",
    "                                    trust_radius=None,\n",
    "                                    fixed_sites=sites)\n",
    "    df_res = pd.DataFrame({\n",
    "        'seq':\n",
    "        seqs.ravel(),\n",
    "        'y':\n",
    "        ys.reshape((num_trajectories, num_iterations)).ravel()\n",
    "    })\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"gb1-paper/gb1_2016_149361_seq.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.AACombo.str.fullmatch('.?DGV')]\n",
    "df2 = df[df.AACombo.str.fullmatch('V.?GV')]\n",
    "df3 = df[df.AACombo.str.fullmatch('VD.?V')]\n",
    "df4 = df[df.AACombo.str.fullmatch('VDG.?')]\n",
    "dftest = pd.concat([df1,df2,df3,df4])\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "#model.eval()\n",
    "if torch.cuda.is_available():\n",
    "  model = model.cuda()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esm(seq):\n",
    "    data = [('0',seq.strip())]\n",
    "\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    if torch.cuda.is_available():\n",
    "      batch_tokens = batch_tokens.to(device=\"cuda\", non_blocking=True)\n",
    "    batch_tokens = batch_tokens.reshape((1,-1))\n",
    "    # Extract per-residue representations\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "    return sequence_representations[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,param_grid,X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    grid = GridSearchCV(model,param_grid,refit=True,cv=3,n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    print(\"score:\",grid.best_score_)\n",
    "    print(\"best_params:\",grid.best_params_)\n",
    "    print(\"#####train set#####\")\n",
    "    report(grid,X_train,y_train)\n",
    "    print(\"#####test set#####\")\n",
    "    report(grid,X_test,y_test)\n",
    "    grid.best_estimator_.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svr hyperparameter tuning\n",
    "svr_param_grid = {'C': [.01,0.1, 1, 10, 100,500, 1000, 5000,10000],\n",
    "                'gamma': [10,100,1000,500,1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "#gpr hyperparameter tuning\n",
    "gpr_param_grid = {'alpha': [1e-10,.01,0.1, 1, 10, 1e-8,1e-6,1e-4,1e-3],'kernel': [RBF(),Matern(),DotProduct()]}\n",
    "\n",
    "class EBO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pool,\n",
    "        init_data,\n",
    "        batch_size=30,\n",
    "        iters=8,\n",
    "    ) -> None:\n",
    "        self.pool = deepcopy(pool)\n",
    "        self.iters = iters\n",
    "        self.batch_size = batch_size\n",
    "        self.train_data = init_data\n",
    "        self.gpr = None\n",
    "        self.svr = None\n",
    "        self.focused_lib = None\n",
    "        self.init_data = init_data\n",
    "\n",
    "    def explore_batch(self):\n",
    "        focused_lib = generate_focused_lib(wt=wt,sites=sites,model=self.svr,num_iterations=30,num_trajectories=300)\n",
    "        focused_lib = focused_lib.groupby('seq').agg('first').sort_values('y',ascending=False).reset_index()\n",
    "        focused_lib = pd.merge(focused_lib,self.pool,on='seq',how='inner')\n",
    "        #print(\"focused_lib\",focused_lib.shape)\n",
    "        ac = Acquisiton(focused_lib)\n",
    "        #sampled_df = ac.random(sample_size=self.batch_size)\n",
    "        X = np.vstack([get_esm(s) for s in focused_lib.seq.values])\n",
    "        y_pre,y_std = self.gpr.predict(X,return_std=True)\n",
    "        sampled_df = ac.ucb_sample(y_pre=y_pre,y_std=y_std, sample_size=self.batch_size, beta=2)\n",
    "        #sampled_df = ac.pi_sample(y_pre=y_pre,y_std=y_std, sample_size=self.batch_size)\n",
    "        #self.pool.drop(index=sampled_df.index, inplace=True)  # update the pool\n",
    "        self.pool = self.pool[~self.pool.seq.isin(sampled_df.seq)] # update the pool\n",
    "        self.pool.reset_index(drop=True, inplace=True)\n",
    "        return sampled_df.reset_index(drop=True)\n",
    "\n",
    "    def run(self):\n",
    "        biggest_v_total = []\n",
    "        self.fit_model()\n",
    "        biggest_v_total.append(self.train_data.Fitness.values.max())\n",
    "        for i in range(self.iters):\n",
    "            chose_data = self.explore_batch()\n",
    "            self.train_data = pd.concat(\n",
    "                (self.train_data, chose_data), ignore_index=True\n",
    "            )\n",
    "            self.fit_model()\n",
    "            biggest_v_total.append(self.train_data.Fitness.values.max())\n",
    "        return biggest_v_total\n",
    "\n",
    "    def fit_model(self):\n",
    "        y = self.train_data.Fitness.values\n",
    "        X = np.vstack([get_esm(s) for s in self.train_data.seq.values])\n",
    "        self.gpr = fit_model(model=GaussianProcessRegressor(n_restarts_optimizer=10), X=X, y=y, param_grid=gpr_param_grid)\n",
    "        self.svr = fit_model(model=SVR(), X=X, y=y, param_grid=svr_param_grid)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = dftest\n",
    "df_pool = df.drop(df_init.index)\n",
    "ebo = EBO(pool=df_pool, init_data=df_init,iters=5,batch_size=30)\n",
    "biggest_v_total = ebo.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(a, k):\n",
    "    idx = np.argpartition(a, -k)[-k:]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svr hyperparameter tuning\n",
    "svr_param_grid = {'C': [.01,0.1, 1, 10, 100,500, 1000, 5000,10000],\n",
    "                'gamma': [10,100,1000,500,1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "#gpr hyperparameter tuning\n",
    "gpr_param_grid = {'alpha': [1e-10,.01,0.1, 1, 10, 1e-8,1e-6,1e-4,1e-3],'kernel': [RBF(),Matern(),DotProduct()]}\n",
    "\n",
    "class EBO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pool,\n",
    "        init_data,\n",
    "        batch_size=30,\n",
    "        iters=8,\n",
    "    ) -> None:\n",
    "        self.pool = deepcopy(pool)\n",
    "        self.iters = iters\n",
    "        self.batch_size = batch_size\n",
    "        self.train_data = init_data\n",
    "        self.gpr = None\n",
    "        self.svr = None\n",
    "        self.focused_lib = None\n",
    "        self.init_data = init_data\n",
    "\n",
    "    def explore_batch(self):\n",
    "        #focused_lib = generate_focused_lib(wt=wt,sites=sites,model=self.svr,num_iterations=30,num_trajectories=300)\n",
    "        #focused_lib = focused_lib.groupby('seq').agg('first').sort_values('y',ascending=False).reset_index()\n",
    "        #focused_lib = pd.merge(focused_lib,self.pool,on='seq',how='inner')\n",
    "        #print(\"focused_lib\",focused_lib.shape)\n",
    "        X1280 = np.vstack(self.pool.esm1280.values)\n",
    "        y_pre = self.svr.predict(X1280)\n",
    "        #focused_lib = self.pool[y_pre < y_pre.mean()]\n",
    "        focused_lib = self.pool.iloc[topk(y_pre, 80000)]\n",
    "        ac = Acquisiton(focused_lib)\n",
    "        sampled_df = ac.random(sample_size=self.batch_size)\n",
    "        #X1280 = np.vstack(focused_lib.esm1280.values)\n",
    "        #y_pre,y_std = self.gpr.predict(X1280,return_std=True)\n",
    "        #sampled_df = ac.ucb_sample(y_pre=y_pre,y_std=y_std, sample_size=self.batch_size, beta=0)\n",
    "        #sampled_df = ac.pi_sample(y_pre=y_pre,y_std=y_std, sample_size=self.batch_size)\n",
    "        #self.pool.drop(index=sampled_df.index, inplace=True)  # update the pool\n",
    "        self.pool = self.pool[~self.pool.seq.isin(sampled_df.seq)] # update the pool\n",
    "        self.pool.reset_index(drop=True, inplace=True)\n",
    "        return sampled_df.reset_index(drop=True)\n",
    "\n",
    "    def run(self):\n",
    "        biggest_v_total = []\n",
    "        self.fit_model()\n",
    "        biggest_v_total.append(self.train_data.Fitness.values.max())\n",
    "        for i in range(self.iters):\n",
    "            chose_data = self.explore_batch()\n",
    "            self.train_data = pd.concat(\n",
    "                (self.train_data, chose_data), ignore_index=True\n",
    "            )\n",
    "            self.fit_model()\n",
    "            biggest_v_total.append(self.train_data.Fitness.values.max())\n",
    "        return biggest_v_total\n",
    "\n",
    "    def fit_model(self):\n",
    "        y = self.train_data.Fitness.values\n",
    "        X = np.vstack(self.train_data.esm1280.values)\n",
    "        #self.gpr = fit_model(model=GaussianProcessRegressor(n_restarts_optimizer=10), X=X, y=y, param_grid=gpr_param_grid)\n",
    "        self.svr = fit_model(model=SVR(), X=X, y=y, param_grid=svr_param_grid)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "df_init = dftest.drop_duplicates(subset=['seq'])\n",
    "df_pool = df[~df.seq.isin(df_init.seq)]\n",
    "for i in range(4):\n",
    "    ebo = EBO(pool=df_pool, init_data=df_init,iters=5,batch_size=50)\n",
    "    biggest_v_total = ebo.run()\n",
    "    res['v'+str(i)] = biggest_v_total\n",
    "res.to_csv('ebo-random-5round-batchsite-50-4diff-exp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
